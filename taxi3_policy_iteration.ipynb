{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf05ab8",
   "metadata": {},
   "source": [
    "# taxi3_policy_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f2cc7",
   "metadata": {},
   "source": [
    "## taxi3_policy_iteration For First Homework Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02048eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy converged at iteration 16\n",
      "\n",
      "Starting Taxi-v3 Simulation...\n",
      "\n",
      "+---------+ \n",
      "|R: | : :G|T\n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 0\n",
      "State ID: 93 → (row=0, col=4, pass_loc=3, dest=1)\n",
      "Passenger location: Blue\n",
      "Destination: Green\n",
      "Action: South ↓ (0)\n",
      "Value Estimate: 24.68\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : |T\n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 1\n",
      "State ID: 193 → (row=1, col=4, pass_loc=3, dest=1)\n",
      "Passenger location: Blue\n",
      "Destination: Green\n",
      "Action: South ↓ (0)\n",
      "Value Estimate: 28.54\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : |T\n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 2\n",
      "State ID: 293 → (row=2, col=4, pass_loc=3, dest=1)\n",
      "Passenger location: Blue\n",
      "Destination: Green\n",
      "Action: South ↓ (0)\n",
      "Value Estimate: 32.82\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : |T\n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 3\n",
      "State ID: 393 → (row=3, col=4, pass_loc=3, dest=1)\n",
      "Passenger location: Blue\n",
      "Destination: Green\n",
      "Action: South ↓ (0)\n",
      "Value Estimate: 37.58\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: |T\n",
      "+---------+ \n",
      "Step: 4\n",
      "State ID: 493 → (row=4, col=4, pass_loc=3, dest=1)\n",
      "Passenger location: Blue\n",
      "Destination: Green\n",
      "Action: West ← (3)\n",
      "Value Estimate: 42.86\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B:T| \n",
      "+---------+ \n",
      "Step: 5\n",
      "State ID: 473 → (row=4, col=3, pass_loc=3, dest=1)\n",
      "Passenger location: Blue\n",
      "Destination: Green\n",
      "Action: Pickup :) (4)\n",
      "Value Estimate: 48.74\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B:T| \n",
      "+---------+ \n",
      "Step: 6\n",
      "State ID: 477 → (row=4, col=3, pass_loc=4, dest=1)\n",
      "Passenger location: In Taxi\n",
      "Destination: Green\n",
      "Action: East → (2)\n",
      "Value Estimate: 55.26\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: |T\n",
      "+---------+ \n",
      "Step: 7\n",
      "State ID: 497 → (row=4, col=4, pass_loc=4, dest=1)\n",
      "Passenger location: In Taxi\n",
      "Destination: Green\n",
      "Action: North ↑ (1)\n",
      "Value Estimate: 62.52\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : |T\n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 8\n",
      "State ID: 397 → (row=3, col=4, pass_loc=4, dest=1)\n",
      "Passenger location: In Taxi\n",
      "Destination: Green\n",
      "Action: North ↑ (1)\n",
      "Value Estimate: 70.57\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : |T\n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 9\n",
      "State ID: 297 → (row=2, col=4, pass_loc=4, dest=1)\n",
      "Passenger location: In Taxi\n",
      "Destination: Green\n",
      "Action: North ↑ (1)\n",
      "Value Estimate: 79.53\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : |T\n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 10\n",
      "State ID: 197 → (row=1, col=4, pass_loc=4, dest=1)\n",
      "Passenger location: In Taxi\n",
      "Destination: Green\n",
      "Action: North ↑ (1)\n",
      "Value Estimate: 89.47\n",
      "Reward received: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G|T\n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 11\n",
      "State ID: 97 → (row=0, col=4, pass_loc=4, dest=1)\n",
      "Passenger location: In Taxi\n",
      "Destination: Green\n",
      "Action: Dropoff ⬇ (5)\n",
      "Value Estimate: 100.53\n",
      "Reward received: 20\n",
      "------------------------------\n",
      "\n",
      "Episode finished in 12 steps with total reward: 9\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "# === Decoding a state into components ===\n",
    "# Observation space is encoded as an integer, decoded here:\n",
    "# ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "# taxi_row: 5 options, taxi_col: 5 options\n",
    "# passenger_location: 5 (Red, Green, Yellow, Blue, In taxi)\n",
    "# destination: 4 (Red, Green, Yellow, Blue)\n",
    "def decode_taxi_state(state):\n",
    "    taxi_row = (state // 100) % 5\n",
    "    taxi_col = (state // 20) % 5\n",
    "    pass_loc = (state // 4) % 5\n",
    "    dest_idx = state % 4\n",
    "    return taxi_row, taxi_col, pass_loc, dest_idx\n",
    "\n",
    "# === Custom map drawing ===\n",
    "# Shows where the taxi is, and includes layout with labeled destinations.\n",
    "def draw_custom_map(taxi_row, taxi_col, pass_loc, dest_idx, carrying):\n",
    "    map_lines = [\n",
    "        list(\"+---------+\"),\n",
    "        list(\"|R: | : :G|\"),  # R and G mark Red and Green destination\n",
    "        list(\"| : | : : |\"),\n",
    "        list(\"| : : : : |\"),\n",
    "        list(\"| | : | : |\"),\n",
    "        list(\"|Y| : |B: |\"),  # Y and B mark Yellow and Blue destination\n",
    "        list(\"+---------+\")\n",
    "    ]\n",
    "    \n",
    "    for i in range(len(map_lines)):\n",
    "        while len(map_lines[i]) < 12:\n",
    "            map_lines[i].append(' ')\n",
    "\n",
    "    row_map = [1, 2, 3, 4, 5]\n",
    "    col_map = [1, 3, 6, 9, 11]\n",
    "\n",
    "    if 0 <= taxi_row < 5 and 0 <= taxi_col < 5:\n",
    "        vis_row = row_map[taxi_row]\n",
    "        vis_col = col_map[taxi_col]\n",
    "        if vis_col < len(map_lines[vis_row]):\n",
    "            taxi_symbol = \"T\" if not carrying else \"T\"\n",
    "            map_lines[vis_row][vis_col] = taxi_symbol\n",
    "\n",
    "    return '\\n'.join(''.join(line) for line in map_lines)\n",
    "\n",
    "# === Policy Iteration (solves the MDP) ===\n",
    "# Covers:\n",
    "# - State transitions\n",
    "# - Reward system: -1 step, -10 illegal, +20 dropoff\n",
    "# - Action effects from env.unwrapped.P\n",
    "def policy_iteration(env, discount_factor=0.9, max_iterations=1000, theta=1e-5):\n",
    "    n_states = env.observation_space.n   # Total of 500 possible states\n",
    "    n_actions = env.action_space.n\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    V = np.zeros(n_states)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # === Policy Evaluation ===\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(n_states):\n",
    "                a = policy[s]\n",
    "                v = 0\n",
    "                for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n",
    "                    v += prob * (reward + discount_factor * V[next_state])\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "                V[s] = v\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        # === Policy Improvement ===\n",
    "        policy_stable = True\n",
    "        for s in range(n_states):\n",
    "            old_action = policy[s]\n",
    "            action_values = np.zeros(n_actions)\n",
    "            for a in range(n_actions):\n",
    "                for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "            best_action = np.argmax(action_values)\n",
    "            policy[s] = best_action\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            print(f\"\\nPolicy converged at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# === Mapping actions to text ===\n",
    "action_names = {\n",
    "    0: \"South ↓\",\n",
    "    1: \"North ↑\",\n",
    "    2: \"East →\",\n",
    "    3: \"West ←\",\n",
    "    4: \"Pickup :)\",\n",
    "    5: \"Dropoff ⬇\"\n",
    "}\n",
    "\n",
    "# === Create environment ===\n",
    "# Covers: Initial state is sampled uniformly from valid set of 300\n",
    "env = gym.make(\"Taxi-v3\", render_mode=None)\n",
    "policy, values = policy_iteration(env)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "print(\"\\nStarting Taxi-v3 Simulation...\\n\")\n",
    "\n",
    "# === Episode simulation loop with goal clarity ===\n",
    "locations = [\"Red\", \"Green\", \"Yellow\", \"Blue\", \"In Taxi\"]\n",
    "\n",
    "while not done and not truncated:\n",
    "    action = policy[observation]\n",
    "    value = values[observation]\n",
    "\n",
    "    # Decode observation\n",
    "    taxi_row, taxi_col, pass_idx, dest_idx = decode_taxi_state(observation)\n",
    "    carrying = pass_idx == 4  # Passenger is in the taxi if pass_loc == 4\n",
    "\n",
    "    # Render custom map\n",
    "    map_output = draw_custom_map(taxi_row, taxi_col, pass_idx, dest_idx, carrying)\n",
    "\n",
    "    # Display step info\n",
    "    print(map_output)\n",
    "    print(f\"Step: {step}\")\n",
    "    print(f\"State ID: {observation} → (row={taxi_row}, col={taxi_col}, pass_loc={pass_idx}, dest={dest_idx})\")\n",
    "    print(f\"Passenger location: {locations[pass_idx]}\")\n",
    "    print(f\"Destination: {locations[dest_idx]}\")\n",
    "    print(f\"Action: {action_names[action]} ({action})\")\n",
    "    print(f\"Value Estimate: {value:.2f}\")\n",
    "\n",
    "    # Take action\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Reward received: {reward}\")\n",
    "    print(f\"{'-'*30}\")\n",
    "    time.sleep(0.7)\n",
    "    step += 1\n",
    "    \n",
    "print(f\"\\nEpisode finished in {step} steps with total reward: {total_reward}\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179c59a",
   "metadata": {},
   "source": [
    "## taxi3_policy_iteration For The Final With Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07d19cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Policy Iteration ---\n",
      "Policy Iteration: Iteration 1 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 2 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 3 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 4 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 5 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 6 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 7 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 8 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 9 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 10 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 11 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 12 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 13 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 14 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 15 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 16 complete. Policy stable: False\n",
      "Policy Iteration: Iteration 17 complete. Policy stable: True\n",
      "\n",
      "Policy converged at iteration 17\n",
      "Policy Iteration Training Time: 3.0774 seconds\n",
      "Policy Iteration Converged in: 17 iterations\n",
      "\n",
      "--- Evaluating Final Policy for 1000 episodes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Policy: 100%|██████████| 1000/1000 [00:00<00:00, 1612.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results ---\n",
      "Average Reward: 7.72 +/- 2.56\n",
      "Average Steps: 13.28 +/- 2.56\n",
      "Success Rate: 100.00%\n",
      "\n",
      "--- Policy Iteration Metrics Summary ---\n",
      "Algorithm: Policy Iteration\n",
      "Iterations to Converge: 17\n",
      "Training Time (s): 3.0773661136627197\n",
      "Final Policy - Avg Reward: 7.72 ± 2.56\n",
      "Final Policy - Avg Steps: 13.28 ± 2.56\n",
      "Final Policy - Success Rate (%): 100.00\n",
      "Avg Reward per Ep (Learning Curve): N/A (Model-based)\n",
      "Steps per Ep (Learning Curve): N/A (Model-based)\n",
      "Success Rate (Learning Curve): N/A (Model-based)\n",
      "\n",
      "--- Simulating one episode with the optimal policy ---\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : |T\n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 0, State ID: 386 → (row=3, col=4, pass_loc=G (Green), dest=Y (Yellow))\n",
      "Action: North ↑ (1), Value Estimate: 835.38\n",
      "Reward: -1, Total Reward: -1\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : |T\n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 1, State ID: 286 → (row=2, col=4, pass_loc=G (Green), dest=Y (Yellow))\n",
      "Action: North ↑ (1), Value Estimate: 844.83\n",
      "Reward: -1, Total Reward: -2\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : |T\n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 2, State ID: 186 → (row=1, col=4, pass_loc=G (Green), dest=Y (Yellow))\n",
      "Action: North ↑ (1), Value Estimate: 854.37\n",
      "Reward: -1, Total Reward: -3\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G|T\n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 3, State ID: 86 → (row=0, col=4, pass_loc=G (Green), dest=Y (Yellow))\n",
      "Action: Pickup :) (4), Value Estimate: 864.01\n",
      "Reward: -1, Total Reward: -4\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G|T\n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 4, State ID: 98 → (row=0, col=4, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: South ↓ (0), Value Estimate: 873.75\n",
      "Reward: -1, Total Reward: -5\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : |T\n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 5, State ID: 198 → (row=1, col=4, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: South ↓ (0), Value Estimate: 883.59\n",
      "Reward: -1, Total Reward: -6\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : |T\n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 6, State ID: 298 → (row=2, col=4, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: West ← (3), Value Estimate: 893.52\n",
      "Reward: -1, Total Reward: -7\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : :T| \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 7, State ID: 278 → (row=2, col=3, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: West ← (3), Value Estimate: 903.56\n",
      "Reward: -1, Total Reward: -8\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : T : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 8, State ID: 258 → (row=2, col=2, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: West ← (3), Value Estimate: 913.69\n",
      "Reward: -1, Total Reward: -9\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| :T: : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 9, State ID: 238 → (row=2, col=1, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: West ← (3), Value Estimate: 923.93\n",
      "Reward: -1, Total Reward: -10\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "|T: : : : | \n",
      "| | : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 10, State ID: 218 → (row=2, col=0, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: South ↓ (0), Value Estimate: 934.28\n",
      "Reward: -1, Total Reward: -11\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "|T| : | : | \n",
      "|Y| : |B: | \n",
      "+---------+ \n",
      "Step: 11, State ID: 318 → (row=3, col=0, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: South ↓ (0), Value Estimate: 944.72\n",
      "Reward: -1, Total Reward: -12\n",
      "------------------------------\n",
      "+---------+ \n",
      "|R: | : :G| \n",
      "| : | : : | \n",
      "| : : : : | \n",
      "| | : | : | \n",
      "|T| : |B: | \n",
      "+---------+ \n",
      "Step: 12, State ID: 418 → (row=4, col=0, pass_loc=In Taxi, dest=Y (Yellow))\n",
      "Action: Dropoff ⬇ (5), Value Estimate: 955.28\n",
      "Reward: 20, Total Reward: 8\n",
      "------------------------------\n",
      "\n",
      "Simulation finished in 13 steps with total reward: 8\n",
      "Successful Dropoff in simulation!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm          \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# === Decoding a state into components ===\n",
    "# Observation space is encoded as an integer, decoded here:\n",
    "# ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "# taxi_row: 5 options, taxi_col: 5 options\n",
    "# passenger_location: 5 (Red, Green, Yellow, Blue, In taxi)\n",
    "# destination: 4 (Red, Green, Yellow, Blue)\n",
    "def decode_taxi_state(state):\n",
    "    taxi_row = (state // 100) % 5\n",
    "    taxi_col = (state // 20) % 5\n",
    "    pass_loc = (state // 4) % 5\n",
    "    dest_idx = state % 4\n",
    "    return taxi_row, taxi_col, pass_loc, dest_idx\n",
    "\n",
    "# === Custom map drawing ===\n",
    "# Shows where the taxi is, and includes layout with labeled destinations.\n",
    "def draw_custom_map(taxi_row, taxi_col, pass_loc, dest_idx, carrying):\n",
    "    map_lines = [\n",
    "        list(\"+---------+\"), list(\"|R: | : :G|\"), list(\"| : | : : |\"),\n",
    "        list(\"| : : : : |\"), list(\"| | : | : |\"), list(\"|Y| : |B: |\"),\n",
    "        list(\"+---------+\")\n",
    "    ]\n",
    "    for i in range(len(map_lines)):\n",
    "        while len(map_lines[i]) < 12: map_lines[i].append(' ')\n",
    "    row_map = [1, 2, 3, 4, 5]\n",
    "    col_map = [1, 3, 6, 9, 11]\n",
    "    if 0 <= taxi_row < 5 and 0 <= taxi_col < 5:\n",
    "        vis_row = row_map[taxi_row]\n",
    "        vis_col = col_map[taxi_col]\n",
    "        if vis_col < len(map_lines[vis_row]):\n",
    "            map_lines[vis_row][vis_col] = \"T\" # Simplified symbol\n",
    "    return '\\n'.join(''.join(line) for line in map_lines)\n",
    "\n",
    "# === Policy Iteration (solves the MDP) ===\n",
    "def policy_iteration(env, discount_factor=0.9, max_iterations=1000, theta=1e-5):\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    policy = np.zeros(n_states, dtype=int) # Initialize with a random/arbitrary policy\n",
    "    V = np.zeros(n_states)\n",
    "    iterations_converged = -1 # To store convergence iteration\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # === Policy Evaluation ===\n",
    "        eval_iter = 0\n",
    "        while True:\n",
    "            eval_iter += 1\n",
    "            delta = 0\n",
    "            for s in range(n_states):\n",
    "                v_old = V[s] # Store old V[s] for delta calculation\n",
    "                a = policy[s]\n",
    "                v_new = 0\n",
    "                # env.P[s][a] is a list of (prob, next_state, reward, done) tuples\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    v_new += prob * (reward + discount_factor * V[next_state])\n",
    "                V[s] = v_new\n",
    "                delta = max(delta, abs(v_new - v_old))\n",
    "            if delta < theta:\n",
    "                # print(f\"Policy Evaluation converged in {eval_iter} iterations for PI iteration {i+1}\")\n",
    "                break\n",
    "            if eval_iter > 10000: \n",
    "                print(f\"Policy Evaluation did not converge within 10000 iterations for PI iteration {i+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "        # === Policy Improvement ===\n",
    "        policy_stable = True\n",
    "        for s in range(n_states):\n",
    "            old_action = policy[s]\n",
    "            action_values = np.zeros(n_actions)\n",
    "            for a__check in range(n_actions): # Renamed 'a' to avoid conflict\n",
    "                for prob, next_state, reward, done in env.P[s][a__check]:\n",
    "                    action_values[a__check] += prob * (reward + discount_factor * V[next_state])\n",
    "            best_action = np.argmax(action_values)\n",
    "            policy[s] = best_action\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        print(f\"Policy Iteration: Iteration {i+1} complete. Policy stable: {policy_stable}\")\n",
    "\n",
    "        if policy_stable:\n",
    "            iterations_converged = i + 1\n",
    "            print(f\"\\nPolicy converged at iteration {iterations_converged}\")\n",
    "            break\n",
    "        if i == max_iterations -1:\n",
    "             print(f\"\\nPolicy Iteration reached max_iterations ({max_iterations}) without full stability.\")\n",
    "             iterations_converged = max_iterations\n",
    "\n",
    "\n",
    "    return policy, V, iterations_converged\n",
    "\n",
    "# === Evaluate Final Policy Performance ===\n",
    "def evaluate_policy(env, policy, num_episodes=1000):\n",
    "    total_rewards = []\n",
    "    total_steps = []\n",
    "    successful_dropoffs = 0\n",
    "\n",
    "    print(f\"\\n--- Evaluating Final Policy for {num_episodes} episodes ---\")\n",
    "    for i in tqdm(range(num_episodes), desc=\"Evaluating Policy\"):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        \n",
    "        # Max steps per evaluation episode to prevent infinite loops from a bad policy\n",
    "        # (though PI should yield an optimal or near-optimal policy)\n",
    "        for _ in range(200): # Taxi-v3 default max_episode_steps is 200\n",
    "            action = policy[observation]\n",
    "            observation, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        total_steps.append(episode_steps)\n",
    "        if done and reward == 20: # Successful dropoff gives +20\n",
    "            successful_dropoffs += 1\n",
    "            \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    std_reward = np.std(total_rewards)\n",
    "    avg_steps = np.mean(total_steps)\n",
    "    std_steps = np.std(total_steps)\n",
    "    success_rate = (successful_dropoffs / num_episodes) * 100\n",
    "\n",
    "    print(f\"--- Evaluation Results ---\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    print(f\"Average Steps: {avg_steps:.2f} +/- {std_steps:.2f}\")\n",
    "    print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "    \n",
    "    return avg_reward, avg_steps, success_rate, std_reward, std_steps\n",
    "\n",
    "# === Mapping actions to text ===\n",
    "action_names = {\n",
    "    0: \"South ↓\", 1: \"North ↑\", 2: \"East →\",\n",
    "    3: \"West ←\", 4: \"Pickup :)\", 5: \"Dropoff ⬇\"\n",
    "}\n",
    "locations = [\"R (Red)\", \"G (Green)\", \"Y (Yellow)\", \"B (Blue)\", \"In Taxi\"]\n",
    "\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    env_pi = gym.make(\"Taxi-v3\", render_mode=None)\n",
    "    raw_env = env_pi.unwrapped     # now raw_env.P exists\n",
    "\n",
    "\n",
    "    # ---  Run Policy Iteration and Collect Training Metrics ---\n",
    "    print(\"--- Starting Policy Iteration ---\")\n",
    "    pi_start_time = time.time()\n",
    "   \n",
    "    optimal_policy, optimal_values, pi_iterations = policy_iteration(raw_env, discount_factor=0.99, theta=1e-6)\n",
    "\n",
    "    pi_training_time = time.time() - pi_start_time\n",
    "    print(f\"Policy Iteration Training Time: {pi_training_time:.4f} seconds\")\n",
    "    print(f\"Policy Iteration Converged in: {pi_iterations} iterations\")\n",
    "\n",
    "    # --- 2Evaluate the Final Policy from Policy Iteration ---\n",
    "    pi_avg_reward, pi_avg_steps, pi_success_rate, pi_std_reward, pi_std_steps = evaluate_policy(env_pi, optimal_policy, num_episodes=1000)\n",
    "\n",
    "    # ---  Store Metrics for Report ---\n",
    "    policy_iteration_metrics = {\n",
    "        \"Algorithm\": \"Policy Iteration\",\n",
    "        \"Iterations to Converge\": pi_iterations,\n",
    "        \"Training Time (s)\": pi_training_time,\n",
    "        \"Final Policy - Avg Reward\": f\"{pi_avg_reward:.2f} ± {pi_std_reward:.2f}\",\n",
    "        \"Final Policy - Avg Steps\": f\"{pi_avg_steps:.2f} ± {pi_std_steps:.2f}\",\n",
    "        \"Final Policy - Success Rate (%)\": f\"{pi_success_rate:.2f}\",\n",
    "       \n",
    "        \"Avg Reward per Ep (Learning Curve)\": \"N/A (Model-based)\",\n",
    "        \"Steps per Ep (Learning Curve)\": \"N/A (Model-based)\",\n",
    "        \"Success Rate (Learning Curve)\": \"N/A (Model-based)\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\n--- Policy Iteration Metrics Summary ---\")\n",
    "    for key, value in policy_iteration_metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # ---  Simulate one episode with the learned policy for visualization ---\n",
    "    print(\"\\n--- Simulating one episode with the optimal policy ---\")\n",
    "    observation, info = env_pi.reset(seed=42) \n",
    "    done = False\n",
    "    truncated = False\n",
    "    total_reward_sim = 0\n",
    "    step_sim = 0\n",
    "\n",
    "    while not done and not truncated and step_sim < 50: # Limit steps for display\n",
    "        action = optimal_policy[observation]\n",
    "        value_sim = optimal_values[observation]\n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = decode_taxi_state(observation)\n",
    "        carrying = pass_idx == 4\n",
    "        map_output = draw_custom_map(taxi_row, taxi_col, pass_idx, dest_idx, carrying)\n",
    "        \n",
    "        print(map_output)\n",
    "        print(f\"Step: {step_sim}, State ID: {observation} → (row={taxi_row}, col={taxi_col}, pass_loc={locations[pass_idx]}, dest={locations[dest_idx]})\")\n",
    "        print(f\"Action: {action_names[action]} ({action}), Value Estimate: {value_sim:.2f}\")\n",
    "        \n",
    "        observation, reward, done, truncated, info = env_pi.step(action)\n",
    "        total_reward_sim += reward\n",
    "        print(f\"Reward: {reward}, Total Reward: {total_reward_sim}\")\n",
    "        print(f\"{'-'*30}\")\n",
    "        time.sleep(0.5)\n",
    "        step_sim += 1\n",
    "    \n",
    "    print(f\"\\nSimulation finished in {step_sim} steps with total reward: {total_reward_sim}\")\n",
    "    if done and reward == 20: print(\"Successful Dropoff in simulation!\")\n",
    "    elif truncated: print(\"Simulation truncated (max steps reached).\")\n",
    "\n",
    "    env_pi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b293b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d090c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GymEnv)",
   "language": "python",
   "name": "gymenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
